{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is the core algorithm for training neural networks, and provide a simple code example in Python.\n",
    "\n",
    "Backpropagation (backward propagation of errors) is a supervised learning algorithm used to train artificial neural networks by minimizing the error between predicted and actual outputs. It works by:\n",
    "\n",
    "1. Forward Pass: Computing the output of the network given an input\n",
    "2. Backward Pass: Calculating the gradient of the loss function with respect to each weight by moving backwards through the network\n",
    "3. Weight Update: Adjusting the weights using the gradients to reduce the error\n",
    "\n",
    "<img src=\"images/backpropagation.png\" width=\"450\" />\n",
    "\n",
    "## Example: Single Neuron with Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions:\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training data: XOR problem\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Initialize weights and bias randomly\n",
    "np.random.seed(1)\n",
    "weights = np.random.random((2, 1))  # 2 inputs, 1 output\n",
    "bias = np.random.random((1, 1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for _ in range(epochs):\n",
    "    # Forward pass\n",
    "    # Input layer -> Output layer (single neuron)\n",
    "    output = sigmoid(np.dot(X, weights) + bias)\n",
    "    \n",
    "    # Calculate error\n",
    "    error = y - output\n",
    "    \n",
    "    # Backward pass\n",
    "    # Calculate adjustments using gradient descent\n",
    "    adjustments = error * sigmoid_derivative(output)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights += learning_rate * np.dot(X.T, adjustments)\n",
    "    bias += learning_rate * np.sum(adjustments)\n",
    "\n",
    "# Test the trained network\n",
    "print(\"Final predictions:\")\n",
    "print(sigmoid(np.dot(X, weights) + bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Network Structure**: \n",
    "   - 2 input neurons\n",
    "   - 1 output neuron with sigmoid activation\n",
    "   - We're trying to learn the XOR function\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Takes inputs (X)\n",
    "   - Multiplies by weights\n",
    "   - Adds bias\n",
    "   - Applies sigmoid activation\n",
    "\n",
    "3. **Backward Pass**:\n",
    "   - Calculates error (difference between target and prediction)\n",
    "   - Computes gradient using the derivative of sigmoid\n",
    "   - Propagates error backwards to update weights\n",
    "\n",
    "4. **Weight Update**:\n",
    "   - Adjusts weights and bias using gradient descent\n",
    "   - Learning rate controls step size\n",
    "\n",
    "The output will be something like:\n",
    "```\n",
    "Final predictions:\n",
    "[[0.015]\n",
    " [0.983]\n",
    " [0.983]\n",
    " [0.017]]\n",
    "```\n",
    "\n",
    "These values approximate the XOR function (0, 1, 1, 0), showing the network has learned the pattern.\n",
    "\n",
    "Key concepts in backpropagation:\n",
    "- **Chain Rule**: Used to compute gradients layer by layer\n",
    "- **Gradient Descent**: Optimizes weights to minimize error\n",
    "- **Learning Rate**: Controls how quickly weights are updated\n",
    "- **Activation Functions**: Introduce non-linearity (sigmoid in this case)\n",
    "\n",
    "This is a simplified example. Real neural networks have:\n",
    "- Multiple layers\n",
    "- More neurons per layer\n",
    "- Different activation functions (ReLU, tanh, etc.)\n",
    "- More sophisticated optimization algorithms (Adam, RMSprop)\n",
    "\n",
    "The same principles apply: compute forward, calculate error, propagate backwards, and update weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
