{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE)\n",
    "\n",
    "A **Mixture of Experts (MoE)** is a machine learning architecture that combines multiple specialized models (called **experts**) and a **gating network** that learns to decide which experts to use for a given input.\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "Instead of using a single monolithic model for all inputs, MoE dynamically selects **a subset of experts** to handle each input, allowing for **sparse computation** and **specialization**. This makes MoE both more **scalable** and **efficient**, especially in large-scale models like language models.\n",
    "\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "An MoE consists of:\n",
    "1. **Experts**: Independent sub-models (usually neural networks) trained to specialize in different parts of the input space.\n",
    "2. **Gating Network**: A learned function that, given an input, assigns **weights** (or makes a hard selection) over the experts.\n",
    "\n",
    "Formally, for input $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^K g_i(\\mathbf{x}) \\cdot E_i(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $E_i(\\mathbf{x})$ is the output of the $i$-th expert\n",
    "- $g_i(\\mathbf{x})$ is the gating function output (e.g., softmax or top-k selection)\n",
    "- $K$ is the total number of experts\n",
    "- The sum is over selected experts (can be sparse)\n",
    "\n",
    "\n",
    "### Types of Gating\n",
    "- **Soft gating**: All experts contribute, weighted by $g_i(\\mathbf{x})$ (similar to attention).\n",
    "- **Hard gating**: Only top-k experts are selected (common in large-scale models for computational efficiency).\n",
    "\n",
    "\n",
    "### Benefits\n",
    "- **Efficiency**: Only a small subset of experts is used per input â†’ reduces compute.\n",
    "- **Scalability**: Models can scale to billions of parameters without needing to activate all of them at once.\n",
    "- **Specialization**: Experts can learn to focus on different features or tasks.\n",
    "\n",
    "\n",
    "### Use Cases\n",
    "- **Large Language Models** (e.g., Google's Switch Transformer, GShard, and recent MoE layers in models like GPT and T5 variants).\n",
    "- **Multitask Learning**: Different tasks can route to different experts.\n",
    "- **Continual Learning**: New experts can be added without retraining the whole model.\n",
    "\n",
    "\n",
    "### Example: Top-2 Gating (used in Switch Transformer)\n",
    "Instead of using all experts, use only the top 2 based on gating scores:\n",
    "\n",
    "1. Compute gating weights $g = \\text{softmax}(W \\mathbf{x})$\n",
    "2. Select top 2 experts by score\n",
    "3. Route input only to those 2 experts\n",
    "4. Combine outputs weighted by gating scores\n",
    "\n",
    "This keeps the model efficient while still leveraging expert specialization.\n",
    "\n",
    "### Example: DeepSeek V2 MoE\n",
    "\n",
    "<img src=\"images/deepseek-moe.png\" alt=\"deepseek_v2_moe\" width=\"500\"/>\n",
    "\n",
    "[DeepSeek V2 MoE](https://arxiv.org/pdf/2405.04434)\n",
    "\n",
    "- Opensource [Mixtral 8x7B](https://ollama.com/library/mixtral): 8 experts, each with 7B parameters.\n",
    "- First introduced in 1991 by Jacobs et al. [View](https://www.researchgate.net/publication/233806999_Adaptive_Mixtures_of_Local_Experts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
