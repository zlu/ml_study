{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic AI\n",
    "\n",
    "\n",
    "# Inference Time Compute\n",
    "Inference time compute refers to the amount of computational resources (such as processing power, memory, and time) required to make predictions using a trained machine learning (ML) or deep learning model.\n",
    "\n",
    "Breakdown of Inference Time Compute:\n",
    "\t1.\tInference – This is the process of using a trained model to make predictions on new data.\n",
    "\t2.\tCompute – This includes the processing power (e.g., CPU, GPU, or specialized hardware like TPUs) needed to perform inference.\n",
    "\n",
    "Key Factors Affecting Inference Time Compute:\n",
    "\t•\tModel Size & Complexity – Larger models (e.g., deep neural networks) require more computations.\n",
    "\t•\tHardware Acceleration – GPUs and TPUs speed up inference compared to CPUs.\n",
    "\t•\tBatch Size – Processing multiple inputs simultaneously can be more efficient.\n",
    "\t•\tQuantization & Optimization – Techniques like model pruning and quantization reduce inference time.\n",
    "\t•\tLatency vs. Throughput – Optimizing for real-time applications (low latency) versus bulk processing (high throughput).\n",
    "\n",
    "Example:\n",
    "\n",
    "A deep learning model like GPT-4 requires significant inference time compute due to its billions of parameters, whereas a small decision tree model has minimal computational requirements.\n",
    "\n",
    "Optimizing inference time compute depends on the model type, hardware, and use case. Here are some key strategies to speed up inference while maintaining accuracy:\n",
    "\n",
    "1. Model Optimization Techniques\n",
    "\n",
    "a) Quantization\n",
    "\t•\tConverts high-precision (e.g., 32-bit floating point) weights to lower precision (e.g., 8-bit integer).\n",
    "\t•\tReduces memory usage and speeds up computations.\n",
    "\t•\tWorks well for mobile and edge AI devices.\n",
    "\t•\tTools: TensorFlow Lite, ONNX Runtime, PyTorch quantization.\n",
    "\n",
    "b) Pruning\n",
    "\t•\tRemoves redundant or insignificant weights in a neural network.\n",
    "\t•\tSpeeds up inference without significant accuracy loss.\n",
    "\t•\tWorks well for compressed model deployment.\n",
    "\n",
    "c) Knowledge Distillation\n",
    "\t•\tTrains a smaller model (student) to mimic a larger one (teacher).\n",
    "\t•\tUsed in cases like transformer models (e.g., TinyBERT, DistilBERT).\n",
    "\t•\tReduces compute requirements with minimal performance drop.\n",
    "\n",
    "2. Hardware Optimization\n",
    "\n",
    "a) Leverage Specialized Accelerators\n",
    "\t•\tGPUs – Suitable for parallel computing and large deep learning models.\n",
    "\t•\tTPUs – Designed for fast AI inference (e.g., Google Cloud TPUs).\n",
    "\t•\tFPGAs & ASICs – Custom chips optimized for low-power inference (e.g., Edge TPU, Nvidia Jetson).\n",
    "\n",
    "b) Use Tensor Cores & Mixed Precision\n",
    "\t•\tNvidia Tensor Cores in RTX and A100 GPUs allow faster matrix multiplications.\n",
    "\t•\tMixed precision (FP16, INT8) helps optimize inference speed.\n",
    "\n",
    "3. Software & Algorithmic Improvements\n",
    "\n",
    "a) Efficient Model Architectures\n",
    "\t•\tUse lightweight models like MobileNet, EfficientNet, or YOLO for vision tasks.\n",
    "\t•\tTransformer-based models like ALBERT (for NLP) are optimized for inference.\n",
    "\n",
    "b) Batch Processing & Parallelism\n",
    "\t•\tRunning inferences in batches (instead of one by one) improves throughput.\n",
    "\t•\tParallelize inference using multi-threading or distributed computing.\n",
    "\n",
    "c) Use Optimized Inference Engines\n",
    "\t•\tTensorRT (for Nvidia GPUs) speeds up model execution.\n",
    "\t•\tONNX Runtime (for cross-platform deployment) optimizes models across different hardware.\n",
    "\t•\tTFLite (for mobile and edge devices) reduces inference time on limited hardware.\n",
    "\n",
    "4. Deployment-Specific Optimizations\n",
    "\n",
    "a) Edge vs. Cloud Inference\n",
    "\t•\tEdge Inference: Low-latency, runs locally on devices (phones, cameras, IoT).\n",
    "\t•\tCloud Inference: More compute power but higher latency due to network overhead.\n",
    "\n",
    "b) Asynchronous Processing\n",
    "\t•\tFor real-time applications, process requests in parallel or asynchronously.\n",
    "\n",
    "5. Case Studies:\n",
    "\t•\tGoogle Assistant uses distilled BERT for fast, low-latency speech processing.\n",
    "\t•\tTesla Autopilot optimizes deep learning inference with custom AI chips.\n",
    "\t•\tStable Diffusion reduces image generation time with optimized tensor computation.\n",
    "\n",
    "# Very Large Language Models\n",
    "> 2 or ~50 trillion parameters\n",
    "\n",
    "# Very Small Language Models\n",
    "a few billion parameters\n",
    "\n",
    "# Human-in-the-Loop Learning/Augmentation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
