{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdd7603-e889-4eb5-a9e0-93b6a08b498f",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "#### Porter Stemmer\n",
    "It is an algorithm for stemming words in NLP.  It reduces words to their root form (stem) by removing suffixes.  It was developed by Martin Porter in 1980.  Using stemmer reduces word variations and improves text processing efficiency.\n",
    "\n",
    "Example: the word \"running\" can be normalized to \"run\", which is the __stem__ of the word.\n",
    "easily -> easi\n",
    "flying -> fli\n",
    "happiness -> happi\n",
    "organization -> organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09be4b7-0d8a-4f31-bd46-02aa585f69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'organ', 'run', 'easili', 'fli', 'happi'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"happiness\", \"happy\", \"organizer\", \"organization\"]\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print results\n",
    "print(set(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8daac4a-e05b-4f13-b6ed-ec99cb22053d",
   "metadata": {},
   "source": [
    "#### List Comprehension\n",
    "\n",
    "<code>\n",
    "[treatment(word) for word in tokens if condition]\n",
    "</code>\n",
    "\n",
    "* It iterates over each word in tokens.\n",
    "* If condition is met\n",
    "* Then applies treatment to the word (such as stemming)\n",
    "* return all treated words in a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e2ebca-2e09-4a4d-8b67-a8bd93d01ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'quickli', 'happi', 'organ']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"running\", \"the\", \"quickly\", \"dogs\", \"happy\", \"is\", \"organization\", \"in\"]\n",
    "stopwords_en = {\"the\", \"is\", \"in\"}\n",
    "\n",
    "# Processing with stemming and filtering\n",
    "# notice that \"dogs\" was filtered\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stopwords_en and len(word) >= 5]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6f376-9ced-4230-a315-3d0acb618364",
   "metadata": {},
   "source": [
    "#### n-gram\n",
    "\n",
    "In NLP, n-gram refer to sequences of words in a text.\n",
    "* Unigram (1-gram) -> Single words\n",
    "* Bigram (2-gram) -> Pairs of consecutive words\n",
    "\n",
    "Example: \"This is a great product\"\n",
    "Unigram -> \"This\", \"is\", \"a\", \"great\", \"product\"\n",
    "Bigram -> \"This is\", \"is a\", \"a great\", \"great product\"\n",
    "\n",
    "Compare to unigram, bigram captures relatiionship between words and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17eedca4-bafd-45b2-ba6a-f18af996a310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zlu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/zlu/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a great product\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Tokenize the text into words\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/zlu/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Example text\n",
    "text = \"This is a great product\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be03f8-9466-4205-9b6c-62d8326b5df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate unigrams (single words)\n",
    "unigrams = tokens\n",
    "\n",
    "# Generate bigrams (pairs of words)\n",
    "bigrams_list = list(bigrams(tokens))\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06465ab4-c244-4bd9-bb8e-f48c47fcfc13",
   "metadata": {},
   "source": [
    "#### Pointwise Mutual Information (PMI)\n",
    "It is a statistical measure used in Natural Language Processing (NLP) to identify how strongly two words are associated with each other.\n",
    "- Measures how much more likely two words appear together compared to chance.\n",
    "- Higher PMI → Stronger association (e.g., \"New York\" appears together often).\n",
    "- Lower PMI → Words appear together randomly (e.g., \"the book\" is common but not meaningful).\n",
    "\n",
    "$$ PMI(x, y) = \\log_2 \\frac{P(x, y)}{P(x) P(y)} $$\n",
    "\n",
    "Where:\n",
    "- $P(x, y)$ = Probability that words x and y appear together\n",
    "- $P(x)$ = Probability of x appearing anywhere\n",
    "- $P(y)$ = Probability of y appearing anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc07020-a08c-4c0e-a407-3c8c297a7bcb",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "<code>\n",
    "\"The battery life is amazing.\"\n",
    "\"This phone has amazing battery performance.\"\n",
    "\"Amazing product with long battery life.\"\n",
    "</code>\n",
    "\n",
    "\"Battery\": count -> 3, Probability 3/15\n",
    "\"amazing\": count -> 3, Probability 3/15\n",
    "\n",
    "__Bigram \"amazing battery\" Appears 2 Times__\n",
    "\n",
    "$PMI(“amazing”, “battery”) = \\log_2 \\frac{P(“amazing”, “battery”)}{P(“amazing”) \\times P(“battery”)}$\n",
    "\n",
    "$PMI(“amazing”, “battery”) = \\log_2 \\frac{2/15}{(3/15) \\times (3/15)}$\n",
    "\n",
    "$PMI(“amazing”, “battery”) = \\log_2 (2.22) = 1.15$\n",
    "\n",
    "Since PMI is positive, \"amazing battery\" is a meaningful phrase.\n",
    "\n",
    "__Why Use PMI for Bigrams?__\n",
    "- Identifies collocations (meaningful word pairs) → \"New York\", \"machine learning\", \"battery life\".\n",
    "- Filters out common but unimportant word pairs → \"the book\", \"in a\", \"this is\".\n",
    "- Used for bigram selection in NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05fd90c3-d17b-4e2c-93a4-0e0d1f95715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Example text corpus\n",
    "corpus = [\n",
    "    \"The battery life is amazing.\",\n",
    "    \"This phone has amazing battery performance.\",\n",
    "    \"Amazing product with long battery life.\"\n",
    "]\n",
    "\n",
    "# Tokenize text\n",
    "tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Count unigrams and bigrams\n",
    "unigram_counts = Counter(word for doc in tokenized_corpus for word in doc)\n",
    "bigram_counts = Counter(bigram for doc in tokenized_corpus for bigram in bigrams(doc))\n",
    "\n",
    "# Total number of words and bigrams\n",
    "total_words = sum(unigram_counts.values())\n",
    "total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "# Calculate PMI for each bigram\n",
    "pmi_scores = {}\n",
    "for bigram, bigram_count in bigram_counts.items():\n",
    "    word_x, word_y = bigram\n",
    "    p_x = unigram_counts[word_x] / total_words\n",
    "    p_y = unigram_counts[word_y] / total_words\n",
    "    p_xy = bigram_count / total_bigrams\n",
    "\n",
    "    # Compute PMI score\n",
    "    pmi_scores[bigram] = math.log2(p_xy / (p_x * p_y))\n",
    "\n",
    "# Print top PMI bigrams\n",
    "sorted_pmi = sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top PMI Bigrams:\", sorted_pmi[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65962590-3630-4ab9-b6ce-c6f280ae645d",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "Vocabulary: {'this': 5, 'is': 2, 'great': 1, 'product': 4, 'amazing': 0, 'love': 3}\n",
    "Sparse Matrix:\n",
    " [[1 1 1 0 1 1]  # \"This is a great product\"\n",
    "  [1 0 1 0 1 1]  # \"This product is amazing\"\n",
    "  [1 1 0 1 1 1]] # \"I love this great product\"\n",
    "\n",
    "- Each row represents a sentence.\n",
    "- Each column corresponds to a word (from vectorizer.vocabulary_).\n",
    "- Numbers indicate word occurrences in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6cec9-2fab-4b98-a989-0b62b89c0670",
   "metadata": {},
   "source": [
    "#### How fit_transform() Works Internally\n",
    "| Text | Tokenized Words | Vectorized Output |\n",
    "| -------- | ------- | -------- |\n",
    "| \"This is a great product\" | ['this', 'is', 'great', 'product'] | [1, 1, 1, 1, 1, 1] |\n",
    "| \"This product is amazing\" | ['this', 'product', 'is', 'amazing'] | [1, 0, 1, 0, 1, 1]|\n",
    "| \"I love this great product\" | ['i', 'love', 'this', 'great', 'product'] |[1, 1, 0, 1, 1, 1] |\n",
    "\n",
    "- fit() learns the vocabulary from the text corpus.\n",
    "- transform() converts text into a sparse matrix of word counts.\n",
    "- Efficient for NLP tasks like text classification, clustering, and topic modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8f7ba-78f4-4711-8b25-f48a05fcc444",
   "metadata": {},
   "source": [
    "#### Bag of Words (BoW)\n",
    "It is a text representation technique in NLP.\n",
    "Where:\n",
    "- Each document (text) is represented as a collection (bag) of __unique__ words.\n",
    "- Ignores word order and grammar.\n",
    "- Counts word occurances to create a numerical feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853be268-2cab-4eb1-af39-4286b896c33c",
   "metadata": {},
   "source": [
    "#### Corpus\n",
    "It is a collection of text documents used for NLP analysis.\n",
    "- A corpus can be a collection of books, news articles, tweets, or product reviews.\n",
    "- In the BoW model, a corpus is used to build the vocabulary.\n",
    "\n",
    "__Example: List of Documents__\n",
    "\n",
    "<pre>\n",
    "corpus = [\n",
    "    \"I love NLP and machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"Deep learning and NLP are the future\"\n",
    "]    \n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98836ce4-d88f-41c4-a874-911d30038fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample Corpus\n",
    "corpus = [\n",
    "    \"I love NLP and machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"Deep learning and NLP are the future\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert text corpus into BoW sparse matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "print(\"BoW Representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d98ed1-a1ae-4033-b24a-1d03e15c5f5b",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "<pre>\n",
    "Vocabulary: ['and', 'amazing', 'are', 'deep', 'future', 'is', 'learning', 'love', 'machine', 'nlp', 'the']\n",
    "BoW Representation:\n",
    " [[1 0 0 0 0 0 1 1 1 1 0]\n",
    "  [0 1 0 0 0 1 1 0 1 0 0]\n",
    "  [1 0 1 1 1 0 1 0 0 1 1]]    \n",
    "</pre>\n",
    "\n",
    "- Each row represents a sentence in vector form.\n",
    "- Each column represents a word from the vocabulary.\n",
    "- Word count is stored in the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67bcf4-743e-416f-ada6-f11a7c41b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
