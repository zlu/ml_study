{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)\n",
    "\n",
    "\n",
    "<img src=\"images/mr-anderson.png\" alt=\"Mr. Anderson\" width=\"200\"/>\n",
    "<img src=\"images/matrix-environment.png\" alt=\"Matrix\" width=\"460\"/>\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an $environment$. The $agent$ receives $rewards$ or penalties based on its $actions$ and aims to maximize the cumulative reward over time. It’s trial-and-error learning, guided by a reward signal, without being explicitly told what to do.\n",
    "\n",
    "Where:\n",
    "- **Agent**: The decision-maker (e.g., a robot, game player).  It is the entire decision-making entity or system (think about Mr. Anderson in the movie Matrix).  It encapsulates the strategic policy, the learning algorithm, and the model.  The model can be thought of as the brain of the agent.\n",
    "- **Environment**: The world the agent interacts with (e.g., a maze, a game). The real `Matrix`.\n",
    "- **State (s)**: The current situation of the agent in the environment.\n",
    "- **Action (a)**: A decision or choice the agent makes in the environment.  E.g., move left, jump.\n",
    "- **Reward (r)**: Feedback from the environment (positive for good actions, negative for bad).  In a maze-solving game, the reward might be +10 points for reaching the goal; the punishment might be -5 points for hitting a wall, etc.\n",
    "- **Policy (π)**: The strategy the agent uses to choose actions based on states.\n",
    "- **Value Function**: Estimates the expected long-term reward for a state or action.\n",
    "\n",
    "The agent explores the environment, updates its policy based on rewards, and balances **exploration** (trying new actions) and **exploitation** (using known good actions). A common algorithm is **Q-learning**, where the agent maintains a Q-table (state-action value pairs) and updates it using the formula:\n",
    "\n",
    "\n",
    "$$Q(s, a) ← Q(s, a) + α [r + γ * max(Q(s', a')) - Q(s, a)]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$: Learning rate.  It controls how quickly the agent learns from the new experience.  It has a range from 0 to 1.  A low learn rate can be more stable but slower to _converge_.  A high learn rate can be faster but more unstable. 0 means it never learns and will keep the initial values. 1 means it completely overwrites old knowledge.  E.g. $new_val = old_val + 0.1 * (target - old_val)$ means only 0.1 or 10% of the new information is incorporated into the new knowledge.  Commonly set to 0.1 to 0.3, meaning we must balance between stable v.s. fast learning.  A note on _converge_: in the context of learning rate in reinformcement learning, _convergence_ means reaching a stable and (near) optimal solution.\n",
    "- $\\gamma$: Discount factor (future reward importance).  It determins how much future rewards matter compared to immediate rewards.  It has a range from 0 to 1.  Low gamma means agent preferes immediate rewards.  High gamma means agent values future rewards almost as much as immediate rewards.  0 means agent cares only about immediate rewards.  1 means agent cares about future rewards as much as immediate rewards.  E.g. $future_val = immediate_reward + 0.9 * next_state_value$ means future rewards are worth 90% of immediate rewards.  Commonly set to 0.9 to 0.99.  Meaning we must balance between short-term v.s. long-term rewards.\n",
    "- $s^\\prime$: Next state\n",
    "- $max(Q(s', a'))$: Best future reward estimate\n",
    "\n",
    "### Q-Learning Example\n",
    "In a 1D grid, an agent moves left or right to reach a goal.  Plot the agent's learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 10:23:06.939 python[15582:4051886] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-31 10:23:06.939 python[15582:4051886] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-table:\n",
      " [[-0.11515023  0.32403892]\n",
      " [-0.02820785  0.5500498 ]\n",
      " [ 0.01057671  0.78023536]\n",
      " [ 0.27210221  0.99484622]\n",
      " [ 0.          0.        ]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Force TkAgg backend for interactive animation\n",
    "\n",
    "# Environment: 1D grid [0, 1, 2, 3, 4], goal at 4\n",
    "states = 5\n",
    "actions = 2  # 0: left, 1: right\n",
    "Q = np.zeros((states, actions))  # Q-table\n",
    "goal = 4\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "episodes = 5  # Reduced for faster animation\n",
    "epsilon = 0.2  # Exploration rate\n",
    "\n",
    "# Data for animation\n",
    "all_states = []\n",
    "all_rewards = []\n",
    "\n",
    "# Q-learning with step tracking\n",
    "for episode in range(episodes):\n",
    "    state = 0  # Start at 0\n",
    "    total_reward = 0\n",
    "    episode_states = [state]\n",
    "    \n",
    "    while state != goal:\n",
    "        action = np.random.choice([0, 1]) if np.random.random() < epsilon else np.argmax(Q[state])\n",
    "        next_state = state - 1 if action == 0 and state > 0 else state + 1 if action == 1 and state < 4 else state\n",
    "        reward = 1 if next_state == goal else -0.1\n",
    "        total_reward += reward\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "        episode_states.append(state)\n",
    "    \n",
    "    all_states.append(episode_states)\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "# Set up the figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios': [1, 2]})\n",
    "\n",
    "# Grid plot (left)\n",
    "ax1.set_title(\"Agent on Grid\")\n",
    "ax1.set_xlim(-0.5, states - 0.5)\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "ax1.set_xticks(range(states))\n",
    "ax1.set_yticks([])\n",
    "ax1.grid(True)\n",
    "agent_dot, = ax1.plot([0], [0], 'ro', markersize=15, label='Agent')  # Initialize with starting position\n",
    "ax1.plot(goal, 0, 'g*', markersize=20, label='Goal')\n",
    "ax1.legend()\n",
    "\n",
    "# Reward plot (right)\n",
    "ax2.set_title(\"Total Reward Over Episodes\")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax2.set_ylabel(\"Total Reward\")\n",
    "ax2.set_xlim(0, episodes - 1)\n",
    "ax2.set_ylim(min(all_rewards) - 0.5, max(all_rewards) + 0.5)\n",
    "reward_line, = ax2.plot([], [], 'b-', label='Reward')\n",
    "ax2.legend()\n",
    "\n",
    "# Animation initialization function\n",
    "def init():\n",
    "    agent_dot.set_data([0], [0])  # Initial agent position\n",
    "    reward_line.set_data([], [])  # Initial empty reward plot\n",
    "    return agent_dot, reward_line\n",
    "\n",
    "# Animation function\n",
    "def animate(frame):\n",
    "    max_steps = max([len(steps) for steps in all_states])\n",
    "    episode = frame // max_steps\n",
    "    step = frame % max_steps\n",
    "    \n",
    "    episode = min(episode, len(all_states) - 1)\n",
    "    step = min(step, len(all_states[episode]) - 1)\n",
    "    \n",
    "    # Update agent position\n",
    "    agent_dot.set_data([all_states[episode][step]], [0])\n",
    "    \n",
    "    # Update reward plot\n",
    "    completed_episodes = min(episode + 1, len(all_rewards))\n",
    "    reward_line.set_data(list(range(completed_episodes)), all_rewards[:completed_episodes])\n",
    "    \n",
    "    return agent_dot, reward_line\n",
    "\n",
    "# Total frames\n",
    "total_frames = episodes * max([len(steps) for steps in all_states])\n",
    "ani = FuncAnimation(fig, animate, frames=total_frames, init_func=init, interval=100, blit=True)\n",
    "\n",
    "# Ensure the animation displays\n",
    "plt.tight_layout()\n",
    "plt.show(block=True)  # Block to keep window open\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation\n",
    "- **Setup**: A 5-state grid (0 to 4), goal at 4. Actions are left (0) or right (1).\n",
    "- **Q-table**: Stores expected rewards for each state-action pair.\n",
    "- **Learning**: The agent randomly explores 20% of the time (ε=0.2) or exploits the best action. The Q-table updates after each step.\n",
    "- **Plot**: Shows total reward per episode, trending upward as the agent learns to reach the goal efficiently.\n",
    "\n",
    "When you run this, the plot will show the agent’s reward improving over time, and the Q-table will reflect higher values for actions leading to the goal (e.g., moving right from state 3). This is RL in action—learning through feedback!\n",
    "\n",
    "## Reinforcement Learning with Human Feedback\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
