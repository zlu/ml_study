{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "$$QUERY -> Vector DB -> Context -> LLM -> Response$$\n",
    "\n",
    "RAG is a machine learning technique that combines document retrieval and generation models to create responses or summaries based on the context of a given query.\n",
    "\n",
    "The basic idea behind RAG is to first retrieve relevant documents from a large corpus of text, such as books, articles, or web pages, using techniques like keyword matching, topic modeling, or deep learning-based methods. These retrieved documents are then used as input for a generation model, such as a transformer-based language model, which synthesizes the information from the retrieved documents to generate a response.\n",
    "\n",
    "RAG has several advantages over traditional generation models, which rely solely on the context provided by the query itself. For example, RAG can provide more accurate and informative responses because it has access to additional contextual information beyond the query itself. It's also able to handle longer and more complex queries that contain multiple topics or concepts.\n",
    "\n",
    "RAG is being used in various applications such as chatbots, virtual assistants, and customer service systems, where it can help improve response accuracy, speed, and relevance.\n",
    "\n",
    "# CAG (Cache-Augmented Generation)\n",
    "\n",
    "$$ Docs -> KV Cache -> LLM -> Response$$\n",
    "\n",
    "Cache-Augmented Generation (CAG) is an approach to improving the performance of language models, particularly those based on transformers, by caching intermediate results during inference.\n",
    "\n",
    "In traditional transformer-based language models, each token in a sequence is processed sequentially, with the model updating its hidden state at each step based on the input tokens and the previous hidden states. This means that information from earlier tokens is stored in the hidden state, which can be used to generate subsequent tokens.\n",
    "\n",
    "CAG improves this process by caching intermediate results during inference, such as attention scores or hidden states, for use in later steps of processing. This allows the model to reuse previously computed information, rather than recomputing it at each step, which can speed up inference and reduce computational costs.\n",
    "\n",
    "The key advantage of CAG over traditional transformer-based language models is that it can improve performance while reducing computational requirements. By caching intermediate results, the model can generate more accurate and coherent text or perform tasks such as machine translation faster and with less memory usage.\n",
    "\n",
    "CAG has various applications in fields such as natural language processing, computer vision, and audio signal processing, where it can be used to improve the efficiency of machine learning models while maintaining performance. It's also being explored as a way to make transformer-based language models more scalable and practical for real-world applications.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
