{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef03ccd8",
   "metadata": {},
   "source": [
    "layout: post\n",
    "title: â€œHow Transformers Really Work: A Deep Dive with Code and Visualsâ€\n",
    "date: 2025-06-25\n",
    "tags:\n",
    "  - Deep Learning\n",
    "  - NLP\n",
    "  - Computer Vision\n",
    "  - Transformers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Transformers have fundamentally reshaped how machines understand sequences. Whether itâ€™s translating between languages, generating coherent text, or even understanding images â€” transformers are behind the scenes. This post offers a comprehensive and intuitive walkthrough of how transformers operate, with code, diagrams, and a little bit of math. \n",
    "\n",
    "## Part 1: One-Hot Encoding and Dot Products\n",
    "\n",
    "To understand transformers, we begin with something simple: representing words as vectors.\n",
    "By applying integer encoding, we can assign each word a unique integer. For example, letâ€™s say we have a vocabulary of the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162d728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'files': 0, 'find': 1, 'my': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7ad86",
   "metadata": {},
   "source": [
    "Please note, the integer assignment is purely accidental.  There is no inherent meaning in the numbers assigned to the words.  Later we will see this is actually a problem as we won't be able to understand the similarity between words, nor are we able to capture the contextual meaning of them.\n",
    "\n",
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5120c20",
   "metadata": {},
   "source": [
    "\n",
    "Each word is represented as a one-hot vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e83379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot(word, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    vec[vocab[word]] = 1\n",
    "    return vec\n",
    "\n",
    "print(one_hot('find', vocab))  # [0, 1, 0]\n",
    "print(one_hot('files', vocab))  # [1, 0, 0]\n",
    "print(one_hot('my', vocab))  # [0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fb537",
   "metadata": {},
   "source": [
    "\n",
    "Why one-hot? Because it gives each word a unique identity without implying any relation between them.\n",
    "\n",
    "Dot Product: Measuring Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00658771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "a = one_hot('find', vocab)\n",
    "b = np.array([0.2, 0.7, 0.8])\n",
    "print(np.dot(a, b))  # 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1fe09",
   "metadata": {},
   "source": [
    "The dot product acts as a lookup, allowing us to compute similarity and perform matrix multiplication, which is the bedrock of neural networks.\n",
    "\n",
    "ðŸ”¢ Visualizing Dot Products\n",
    "\n",
    "flowchart LR\n",
    "  A[\"Find (0,1,0)\"] -- Dot --> B[\"Vector (0.2, 0.7, 0.8)\"] --> C[Output: 0.7]\n",
    "\n",
    "\n",
    "## Part 2: Sequence Modeling with Markov Chains\n",
    "\n",
    "Letâ€™s consider a user input like: \"show me my files\"\n",
    "\n",
    "A first-order Markov model assumes the next word only depends on the current word:\n",
    "```python\n",
    "transition_probs = {\n",
    "    'my': {'files': 0.3, 'photos': 0.5, 'directories': 0.2}\n",
    "}\n",
    "```\n",
    "We can model this as a matrix:\n",
    "```python\n",
    "matrix = np.array([\n",
    "    # files, photos, directories\n",
    "    [0.3, 0.5, 0.2]\n",
    "])\n",
    "```\n",
    "However, this model canâ€™t capture longer-range dependencies â€” such as knowing that â€œitâ€ refers to â€œthe dogâ€ from earlier in the sentence.\n",
    "\n",
    "\n",
    "## Part 3: Embeddings â€” Compressing Semantics\n",
    "\n",
    "One-hot vectors are sparse and unintelligent. To give them meaning, we embed them into a dense space:\n",
    "```python\n",
    "import torch\n",
    "embedding = torch.nn.Embedding(num_embeddings=50000, embedding_dim=512)\n",
    "word_ids = torch.LongTensor([1, 2, 0])\n",
    "embedded = embedding(word_ids)\n",
    "```\n",
    "This maps each word to a 512-dimensional vector, where semantic closeness is preserved.\n",
    "\n",
    "### Why Embeddings Work\n",
    "\tâ€¢\tWords like â€œParisâ€ and â€œLondonâ€ end up close.\n",
    "\tâ€¢\tEmbedding matrix W learns during training.\n",
    "\tâ€¢\tShape: [vocab_size x embedding_dim]\n",
    "\n",
    "graph TD\n",
    "  A[\"One-hot word\"] -->|W| B[\"Embedded word vector\"]\n",
    "\n",
    "\n",
    "\n",
    "## Part 4: Positional Encoding â€” Giving Order to Words\n",
    "\n",
    "Unlike RNNs, transformers donâ€™t have a natural sense of sequence. So we encode position manually:\n",
    "```python\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = torch.arange(seq_len).unsqueeze(1)\n",
    "    i = torch.arange(d_model).unsqueeze(0)\n",
    "    angle_rates = 1 / torch.pow(10000, (2 * (i//2)) / d_model)\n",
    "    angle_rads = pos * angle_rates\n",
    "    \n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "    return pe\n",
    "```\n",
    "These sinusoidal patterns let the model learn relative positions, essential for grammatical structure.\n",
    "\n",
    "### Positional Encoding Visualization\n",
    "\n",
    "Each row (position) contains a unique combination of sines and cosines:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "  A[Position 0] -->|sin/cos| B[Encoding Vector]\n",
    "  C[Position 1] -->|sin/cos| D[Encoding Vector]\n",
    "```\n",
    "\n",
    "\n",
    "## Part 5: Scaled Dot-Product Attention\n",
    "\n",
    "The magic of transformers is the attention mechanism. It lets the model focus on relevant parts of the input:\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Code:\n",
    "```python\n",
    "def attention(Q, K, V, mask=None):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k**0.5\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, V)\n",
    "```\n",
    "Where:\n",
    "\tâ€¢\tQ: What weâ€™re looking for\n",
    "\tâ€¢\tK: What we have\n",
    "\tâ€¢\tV: What we return if we match\n",
    "\n",
    "\n",
    "## Part 6: Multi-Head Attention\n",
    "\n",
    "Instead of one attention score, we use multiple attention heads:\n",
    "```python    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_linear(q).view(bs, -1, self.heads, self.d_k).transpose(1,2)\n",
    "        K = self.k_linear(k).view(bs, -1, self.heads, self.d_k).transpose(1,2)\n",
    "        V = self.v_linear(v).view(bs, -1, self.heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        concat = output.transpose(1,2).contiguous().view(bs, -1, self.heads * self.d_k)\n",
    "        return self.out(concat)\n",
    "```\n",
    "\n",
    "Diagram\n",
    "\n",
    "flowchart TD\n",
    "  A[Query] --> B(Multi-head Attention)\n",
    "  B --> C[Context Vectors]\n",
    "\n",
    "\n",
    "## Part 7: Feed Forward + Residual Connections\n",
    "\n",
    "Each tokenâ€™s representation is passed through a feedforward network:\n",
    "\n",
    "```python\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "```\n",
    "And wrapped with residual connection + normalization:\n",
    "\n",
    "```python\n",
    "x = x + self.dropout(self.ff(x))\n",
    "x = self.norm(x)\n",
    "```\n",
    "\n",
    "\n",
    "## Part 8: Full Encoder-Decoder Architecture\n",
    "\n",
    "Translation Pipeline:\n",
    "\n",
    "graph LR\n",
    "  A[Input: \"I am good\"] --> B[Encoder]\n",
    "  B --> C[Context Vectors]\n",
    "  D[Decoder] --> E[Output: \"Je vais bien\"]\n",
    "  C --> D\n",
    "\n",
    "During training:\n",
    "\tâ€¢\tThe decoder sees the entire target sentence.\n",
    "During inference:\n",
    "\tâ€¢\tIt sees only previous outputs, one word at a time.\n",
    "\n",
    "\n",
    "## Part 9: Vision Transformers (ViT)\n",
    "\n",
    "Images are split into patches â†’ tokens!\n",
    "```python\n",
    "def image_to_patches(img, patch_size=16):\n",
    "    B, C, H, W = img.shape\n",
    "    img = img.view(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    return img.permute(0,2,4,3,5,1).reshape(B, -1, patch_size*patch_size*C)\n",
    "```\n",
    "Then fed through the same transformer encoder architecture.\n",
    "\n",
    "ViT Diagram\n",
    "\n",
    "graph LR\n",
    "  A[Image] --> B[Patchify + Embed]\n",
    "  B --> C[Transformer Encoder]\n",
    "  C --> D[Classification Head]\n",
    "\n",
    "\n",
    "## Part 10: Fine-Tuning a Transformer (Hugging Face)\n",
    "\n",
    "Hereâ€™s how to fine-tune a transformer (like BERT) for sentiment analysis:\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "training_args = TrainingArguments(\"./bert-finetuned\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'].shuffle(seed=42).select(range(5000)),\n",
    "    eval_dataset=dataset['test'].shuffle(seed=42).select(range(1000))\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "``\n",
    "\n",
    "## Part 11: Visualizing Transformers with Gradio\n",
    "\n",
    "Letâ€™s use Gradio to visualize how attention works.\n",
    "```python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def classify(text):\n",
    "    return pipe(text)\n",
    "\n",
    "demo = gr.Interface(fn=classify, inputs=\"text\", outputs=\"label\")\n",
    "demo.launch()\n",
    "```\n",
    "You can extend this with heatmaps to visualize token-level attention weights.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Transformers build intelligent understanding of sequences â€” whether text or images â€” through:\n",
    "\tâ€¢\tEmbedding\n",
    "\tâ€¢\tPositional Encoding\n",
    "\tâ€¢\tAttention\n",
    "\tâ€¢\tMulti-head parallelism\n",
    "\tâ€¢\tFeedforward networks\n",
    "\n",
    "Theyâ€™ve replaced RNNs in NLP and now rival CNNs in vision. The next frontier? Efficient, universal models across all modalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Fine-Tuning + Gradio Demo\n",
    "This notebook fine-tunes a BERT model on the IMDb dataset and builds a simple Gradio demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaabcf20c04420285fa824d2f65f72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f5aec2e1fc4f75a23adfd2d175af7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2252f28f4a3646adbc11566e301959fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec791d1f21174690bc5c5792fb861415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288e3fa14dd6495a949ec90ced2aca1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001c027626d5434d866540a9c8532bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637844f9457b4a0ba1745a6e0028ea5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd621ccfb904114a4bb519239b11fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4c7a5109f542758855ca8324a9c140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d02c2c4681403891f6fd240e877993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dcc52e66fa47f9819bca9925206252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57762fc2d1b343d3bfa5dbb2cd019db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e72d8b9d0c4e349e517f5cc7b7a87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f934aec313c4d3ba35111280a9c795b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset and tokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df618389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BertIMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Only keep the fields needed for BERT\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.dataset[idx]['input_ids']),\n",
    "            'attention_mask': torch.tensor(self.dataset[idx]['attention_mask']),\n",
    "            'labels': torch.tensor(self.dataset[idx]['label'])\n",
    "        }\n",
    "        return item\n",
    "\n",
    "train_dataset = BertIMDBDataset(dataset['train'])\n",
    "eval_dataset = BertIMDBDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Assume you have train_dataset and eval_dataset as PyTorch Dataset objects\n",
    "# Example: train_dataset = MyCustomDataset(...)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # Assume batch is a dict with 'input_ids', 'attention_mask', 'labels'\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "    # Evaluation loop (optional)\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Validation accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Reload the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert-finetuned\")\n",
    "tokenizer.save_pretrained(\"./bert-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Gradio Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"./bert-finetuned\")\n",
    "\n",
    "def classify(text):\n",
    "    return pipe(text)\n",
    "\n",
    "demo = gr.Interface(fn=classify, inputs=\"text\", outputs=\"label\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization with BERTViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bertviz\n",
    "!pip install bertviz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"The dog chased the cat because it was fast.\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Launch attention head visualization\n",
    "head_view(outputs.attentions, tokens=tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
